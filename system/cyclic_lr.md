# Use cyclic learning rate to arrive at superconvergence
The source is adapted from: \

1. [sgugger](https://sgugger.github.io/the-1cycle-policy.html)
2. [Medium - Adaptive - and Cyclical Learning Rates using PyTorch](https://towardsdatascience.com/adaptive-and-cyclical-learning-rates-using-pytorch-2bf904d18dee)

## Code
We can use pytorch original cyclical learning rate scheduler


## Ref
1. [A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, momentum, and weight decay](https://arxiv.org/abs/1803.09820)
2. [Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates](https://arxiv.org/abs/1708.07120)
3. [Cyclical Learning Rates for Training Neural Networks](https://arxiv.org/abs/1506.01186)
